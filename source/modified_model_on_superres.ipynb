{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "import tensorflow as tf\n",
    "from torchvision.models import vgg16_bn\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import measure\n",
    "\n",
    "from  modified_model_fx import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = ([\n",
    "    RandTransform(tfm=TfmCrop(crop_pad),\n",
    "                  kwargs={\n",
    "                      'row_pct': (0, 1),\n",
    "                      'col_pct': (0, 1),\n",
    "                      'padding_mode': 'reflection'\n",
    "                  },\n",
    "                  p=1.0,\n",
    "                  resolved={},\n",
    "                  do_run=True,\n",
    "                  is_random=True,\n",
    "                  use_on_y=True),\n",
    "    RandTransform(tfm=TfmPixel(flip_lr),\n",
    "                  kwargs={},\n",
    "                  p=0.5,\n",
    "                  resolved={},\n",
    "                  do_run=True,\n",
    "                  is_random=True,\n",
    "                  use_on_y=True),\n",
    "    RandTransform(tfm=TfmCoord(symmetric_warp),\n",
    "                  kwargs={'magnitude': (-0.2, 0.2)},\n",
    "                  p=0.75,\n",
    "                  resolved={},\n",
    "                  do_run=True,\n",
    "                  is_random=True,\n",
    "                  use_on_y=True),\n",
    "    RandTransform(tfm=TfmLighting(brightness),\n",
    "                  kwargs={'change': (0.4, 0.6)},\n",
    "                  p=0.75,\n",
    "                  resolved={},\n",
    "                  do_run=True,\n",
    "                  is_random=True,\n",
    "                  use_on_y=True),\n",
    "    RandTransform(tfm=TfmLighting(contrast),\n",
    "                  kwargs={'scale': (0.8, 1.25)},\n",
    "                  p=0.75,\n",
    "                  resolved={},\n",
    "                  do_run=True,\n",
    "                  is_random=True,\n",
    "                  use_on_y=True)\n",
    "], [\n",
    "    RandTransform(tfm=TfmCrop(crop_pad),\n",
    "                  kwargs={},\n",
    "                  p=1.0,\n",
    "                  resolved={},\n",
    "                  do_run=True,\n",
    "                  is_random=True,\n",
    "                  use_on_y=True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizes of images: \n",
    "\n",
    "- HR images: **(3, 502, 672)** original (3, 1004, 1344) cut in 4 pieces\n",
    "- LR images: **(3, 250, 334)** original (3, 500, 669) cut in 4 pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../../../../../SCRATCH2/marvande/data/train/HR/')\n",
    "\n",
    "path_lr = path / 'small-250/train'\n",
    "path_mr = path / 'small-502/train'\n",
    "\n",
    "path_hr = path / 'HR_patches_train/jpg_images'\n",
    "path_lr_or = path/'HR_patches_resized/jpg_images'\n",
    "\n",
    "assert path.exists(), f\"need dataset @ {path}\"\n",
    "assert path_hr.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = ImageList.from_folder(path_hr)\n",
    "ImageList.from_folder(path_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageList.from_folder(path_lr_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_one(fn, i, path, size):\n",
    "    dest = path/fn.relative_to(path_hr)\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    img = PIL.Image.open(fn)\n",
    "    #targ_sz = resize_to(img, size, use_min=True)\n",
    "    img = img.resize(size, resample=PIL.Image.BILINEAR).convert('RGB')\n",
    "    img.save(dest, quality=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller image sets the first time this nb is run:\n",
    "sets = [(path_lr, (334,250)), (path_mr, (672,502))]\n",
    "for p, size in sets:\n",
    "    if not p.exists():\n",
    "        print(f\"resizing to {size} into {p}\")\n",
    "        parallel(partial(resize_one, path=p, size=size), il.items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageList.from_folder(path_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageList.from_folder(path_mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates two set of images from the HR images:\n",
    "- HR: (3, 502, 672)\n",
    "- LR: (3, 250, 334)\n",
    "- MR: (3, 502, 672)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image size and batch size to which data is transformed:\n",
    "bs, size = 15, (250, 334)\n",
    "arch = models.resnet34\n",
    "\n",
    "src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data without size and other transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.label_from_func(lambda x: path_hr / x.relative_to(path_lr)).databunch())\n",
    "print(data)\n",
    "print('LR (left) vs HR (right) data')\n",
    "data.show_batch(rows=1, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With transformations, resized HR image to the same size as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With transformations, resized HR image to the same size as LR\n",
    "data = (src.label_from_func(lambda x: path_hr/x.relative_to(path_lr)).transform(tfms, size=size, tfm_y=True)\n",
    "            .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n",
    "data.c = 3\n",
    "print(data)\n",
    "print('LR (left) vs HR (right) data')\n",
    "data.show_batch(rows=1, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that does both and creates training and validation data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(bs, size):\n",
    "    #label_from_func: apply func to every input to get its label.\n",
    "    # defining a custom function to extract the labels \n",
    "    data = src.label_from_func(lambda x: path_hr / x.relative_to(path_lr))\n",
    "    \n",
    "    #apply any data transformations, \n",
    "    #i.e. data augmentation techniques to help avoid overfitting during training\n",
    "    data = data.transform(\n",
    "            tfms, size=size,\n",
    "            tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n",
    "    data.c = 3\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data of the shape (3, 250, 334):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(bs,size)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data.valid_ds[0][1].data\n",
    "t = torch.stack([t,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    n,c,h,w = x.size()\n",
    "    x = x.view(n, c, -1)\n",
    "    return (x @ x.transpose(1,2))/(c*h*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrix(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = F.l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_m = vgg16_bn(True).features.cuda().eval()\n",
    "requires_grad(vgg_m, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n",
    "blocks, [vgg_m[i] for i in blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __init__(self, m_feat, layer_ids, layer_wgts):\n",
    "        super().__init__()\n",
    "        self.m_feat = m_feat\n",
    "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
    "        self.hooks = hook_outputs(self.loss_features, detach=False)\n",
    "        self.wgts = layer_wgts\n",
    "        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n",
    "              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n",
    "\n",
    "    def make_features(self, x, clone=False):\n",
    "        self.m_feat(x)\n",
    "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        out_feat = self.make_features(target, clone=True)\n",
    "        in_feat = self.make_features(input)\n",
    "        self.feat_losses = [base_loss(input,target)]\n",
    "        #feat losses\n",
    "        self.feat_losses += [base_loss(f_in, f_out)*w\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        #gram: \n",
    "        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
    "        return sum(self.feat_losses)\n",
    "    \n",
    "    def __del__(self): self.hooks.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 1e-3\n",
    "lr = 1e-3\n",
    "def do_fit(save_name, lrs=slice(lr), pct_start=0.9):\n",
    "    learn.fit_one_cycle(10, lrs, pct_start=pct_start)\n",
    "    learn.save(save_name)\n",
    "    learn.show_results(rows=1, imgsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all tensors and free cache:\n",
    "for obj in gc.get_objects():\n",
    "    if torch.is_tensor(obj):\n",
    "        del obj\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "#learn.destroy()\n",
    "\n",
    "#get free memory (in MBs) for the currently selected gpu id, after emptying the cache\n",
    "print(\n",
    "    'free memory (in MBs) for the currently selected gpu id, after emptying the cache: ',\n",
    "    gpu_mem_get_free_no_cache())\n",
    "\n",
    "print(\n",
    "    'used memory (in MBs) for the currently selected gpu id, after emptying the cache:',\n",
    "    gpu_mem_get_used_no_cache())\n",
    "\n",
    "gpu_mem_get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-3\n",
    "learn = unet_learner(data,\n",
    "                     arch,\n",
    "                     wd=wd,\n",
    "                     loss_func=feat_loss,\n",
    "                     callback_fns=LossMetrics,\n",
    "                     blur=True,\n",
    "                     norm_type=NormType.Weight)\n",
    "# garbage collection:\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fit('1a_mod', slice(lr*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fit('1b_mod', slice(1e-5,lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "#learn.destroy()\n",
    "\n",
    "#get free memory (in MBs) for the currently selected gpu id, after emptying the cache\n",
    "print(\n",
    "    'free memory (in MBs) for the currently selected gpu id, after emptying the cache: ',\n",
    "    gpu_mem_get_free_no_cache())\n",
    "\n",
    "print(\n",
    "    'used memory (in MBs) for the currently selected gpu id, after emptying the cache:',\n",
    "    gpu_mem_get_used_no_cache())\n",
    "\n",
    "gpu_mem_get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = (502, 672)\n",
    "bs = 4\n",
    "data = get_data(bs,new_size)\n",
    "\n",
    "wd = 1e-3\n",
    "learn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,\n",
    "                     blur=True, norm_type=NormType.Weight)\n",
    "\n",
    "lr = 1e-3\n",
    "# garbage collection: \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data\n",
    "learn.freeze()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('1b_mod');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fit('2a_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fit('2b_mod', slice(1e-6,1e-4), pct_start=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the learn object, this time with l1_loss and not with feature_loss, **why?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = None\n",
    "gc.collect()\n",
    "learn = unet_learner(data,\n",
    "                     arch,\n",
    "                     loss_func=F.l1_loss,\n",
    "                     blur=True,\n",
    "                     norm_type=NormType.Weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the sizes of the LR input images (`size_lr`) for testing and the HR images (`size_mr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mr = (3, 502, 672)\n",
    "size_lr = (3, 250, 334)\n",
    "\n",
    "#Check free GPU RAM:\n",
    "free = gpu_mem_get_free_no_cache()\n",
    "print(f\"using size={size}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create testing data, `data_mr` of size HR and `data_lr` of size LR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)\n",
    "          .label_from_func(lambda x: path_hr/x.name)\n",
    "          .transform(tfms, size=size_mr, tfm_y=True)\n",
    "          .databunch(bs=1).normalize(imagenet_stats, do_y=True))\n",
    "data_mr.c = 3\n",
    "data_lr = (ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)\n",
    "          .label_from_func(lambda x: path_hr/x.name)\n",
    "          .transform(tfms, size=size_lr, tfm_y=True)\n",
    "          .databunch(bs=1).normalize(imagenet_stats, do_y=True))\n",
    "data_lr.c = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the learn object from the last phase (`2b_mod`) and set it's data to `data_mr` (**why?**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('2b_mod')\n",
    "learn.data = data_mr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the images we are going to use for testing from `data_mr` and `data_lr`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth HR image:\n",
    "gt_HR = data_mr.valid_ds.y.items[1]\n",
    "\n",
    "# LR version of the same image:\n",
    "lr = data_lr.valid_ds.x.items[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to apply the model and predict the HR image from the LR, we need to resample the LR image to be of the same size as the HR, as the model inputs and outputs images of the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_data = open_image(lr)\n",
    "\n",
    "# resample to same size as HR:\n",
    "# for pytorch, have to add a first new dimension to indicate bs = 1\n",
    "# now lr_data of shape [1, 3, 250, 334]\n",
    "lr_data = lr_data.data.unsqueeze(0)\n",
    "lr_resized = torch.nn.functional.interpolate(lr_data, size_mr[1:],\n",
    "                                             mode='bicubic')\n",
    "# remove the previous added dimension\n",
    "lr_resized = lr_resized.squeeze()\n",
    "\n",
    "# plot resized LR: \n",
    "plot_single_image(lr_resized, \n",
    "                  'Resized LR from size {} to size {}'\\\n",
    "                  .format(list(lr_data.squeeze().shape), \n",
    "                          list(lr_resized.shape)), (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have resized the LR to the same size as the ground truth HR, we can feed it to the model and predict a new HR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth HR: \n",
    "im_HR_gt = open_image(gt_HR)\n",
    "#print('HR ground thruth shape: {}'.format(list(im_HR_gt.shape)))\n",
    "\n",
    "# Prediction of model: \n",
    "p, img_pred, b = learn.predict(Image(lr_resized))\n",
    "#print('Reconstructed HR shape: {}'.format(list(p.shape)))\n",
    "\n",
    "# Assert reconstructed HR has same shape as ground truth HR:\n",
    "assert(list(p.shape) == list(im_HR_gt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this predicted image to the ground truth HR and LR: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3_images(lr_resized, im_HR_gt, img_pred, (20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate loss and error metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans1 = transforms.ToTensor()\n",
    "trans = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images_metrics(trans1(trans(im_HR_gt.data)).numpy(), img_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
